{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_lex_norm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "crqxq4LHvWj_"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore',category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "MAX_LEN = 50"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGiIxAqq6Ik2"
      },
      "source": [
        "def tokenize(data,wordToNum):\n",
        "\n",
        "    tokenizedData = []\n",
        "\n",
        "    for sent in data:\n",
        "\n",
        "        tokenizedSent = []\n",
        "        for word in sent:\n",
        "            tokenizedSent.append(wordToNum[word])\n",
        "\n",
        "        tokenizedSent=np.array(tokenizedSent,dtype=float)\n",
        "        tokenizedData.append(tokenizedSent)\n",
        "\n",
        "    return np.array(tokenizedData)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gIVFdBM6LAc"
      },
      "source": [
        "def predict(raw):\n",
        "    \n",
        "    model = keras.models.load_model('model.h5')\n",
        "    wordToNum = pkl.load(open('data/wordToNum.pkl', 'rb'))\n",
        "    numToWord = pkl.load(open('data/numToWord.pkl', 'rb'))\n",
        "\n",
        "    test = raw.split(\" \")\n",
        "\n",
        "    n = len(test)\n",
        "\n",
        "    for i in range(MAX_LEN-len(test)):\n",
        "        test.append(\"\")\n",
        "\n",
        "    test = [test]\n",
        "    test = tokenize(test, wordToNum)\n",
        "\n",
        "\n",
        "    pred = model.predict(test)\n",
        "\n",
        "    normalized = []\n",
        "    for i in range(MAX_LEN):\n",
        "        normalized.append(numToWord[np.argmax(pred[0][i])])\n",
        "\n",
        "    normalized = \" \".join(normalized[:n])\n",
        "\n",
        "    return normalized"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5BBZcKq6LSl",
        "outputId": "1a92ff1a-8c47-48e4-c4cc-0ce035270da6"
      },
      "source": [
        "raw = \"U r not playin wif Me\"\n",
        "\n",
        "norm = predict(raw)\n",
        "print(\"Raw : \", raw)\n",
        "print(\"Norm: \", norm)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw :  U r not playin wif Me\n",
            "Norm:  you are not playing with me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWtbiKku6NQh",
        "outputId": "a2618098-ba45-416d-e7bc-df1957966545"
      },
      "source": [
        "raw = \"omg u r so funny , LOL\"\n",
        "\n",
        "norm = predict(raw)\n",
        "print(\"Raw : \", raw)\n",
        "print(\"Norm: \", norm)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw :  omg u r so funny , LOL\n",
            "Norm:  oh my god you are so funny , laughing out loud\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_uZ2W0C6O92",
        "outputId": "350ac3a8-e8ed-4028-8cc3-96d77b03841f"
      },
      "source": [
        "raw = \"tbh idk anything abt it\"\n",
        "\n",
        "norm = predict(raw)\n",
        "print(\"Raw : \", raw)\n",
        "print(\"Norm: \", norm)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw :  tbh idk anything abt it\n",
            "Norm:  to be honest i don't know anything about it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e3w3lSi6QPz",
        "outputId": "a525c518-8ea3-4a9b-f009-373febafe05d"
      },
      "source": [
        "raw = \"yea ... didnt Look lik it\"\n",
        "\n",
        "norm = predict(raw)\n",
        "print(\"Raw : \", raw)\n",
        "print(\"Norm: \", norm)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw :  yea ... didnt Look lik it\n",
            "Norm:  yeah ... didn't look like it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKTdPMNuPJ1X",
        "outputId": "5af230a3-69f1-41cf-ee53-26e7765d331c"
      },
      "source": [
        "raw = \"This is a C A T\"\n",
        "\n",
        "norm = predict(raw)\n",
        "print(\"Raw : \", raw)\n",
        "print(\"Norm: \", norm)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88a7bf73b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Raw :  This is a C A T\n",
            "Norm:  this is a c a t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SFBbCfGRDFI"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}